[{"categories":["learn-notes"],"content":"bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器.","date":"2025-08-26","objectID":"/learn-documentation-happy-llm1/","tags":["LLM"],"title":"Happy_LLM_01 NLP前言","uri":"/learn-documentation-happy-llm1/"},{"categories":["learn-notes"],"content":"Task01：项目介绍 + 前言 （这是笔者自己的学习记录，仅供参考，原始学习链接见最下面，愿 LLM 越来越好❤ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm1/:0:0","tags":["LLM"],"title":"Happy_LLM_01 NLP前言","uri":"/learn-documentation-happy-llm1/"},{"categories":["learn-notes"],"content":"1. NLP 主要研究什么？ NLP（Natural Language Processing，自然语言处理） 主要聚焦：计算机如何 理解、处理、生成 人类的语言。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm1/:1:0","tags":["LLM"],"title":"Happy_LLM_01 NLP前言","uri":"/learn-documentation-happy-llm1/"},{"categories":["learn-notes"],"content":"2. LLM vs PLM：两种模型分别是什么？ 简称 全称 中文 时代定位 LLM Large Language Model 大语言模型 当下最火的模型，NLP的衍生成果 PLM Pretrain Language Model 预训练语言模型 NLP 过去的主流模型 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm1/:2:0","tags":["LLM"],"title":"Happy_LLM_01 NLP前言","uri":"/learn-documentation-happy-llm1/"},{"categories":["learn-notes"],"content":"3. LLM 在 PLM 基础上有什么改进？ 维度 PLM（如 BERT、GPT-1/2） LLM（如 GPT-3/4、Qwen、ChatGLM 等） 训练数据规模 相对较小 海量数据 参数量 百万~十亿级 十亿~千亿级 微调方式 需要一定量的监督数据 指令微调 + RLHF（人类反馈强化学习） 能力特征 单一任务表现好 涌现能力（Emergent Ability） - 上下文学习（In-context Learning） - 指令理解（Instruction Following） - 高质量文本生成 一句话总结： 模型更大（参数量大了） + 数据更多（预训练数据规模） + 训练策略更先进 ⇒ LLM 能力“chua”一下爆发！ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm1/:3:0","tags":["LLM"],"title":"Happy_LLM_01 NLP前言","uri":"/learn-documentation-happy-llm1/"},{"categories":["learn-notes"],"content":"4. Datawhale 相关开源项目一览 项目名称 定位 在线地址 Self-LLM （开源大模型食用指南） 为开发者提供一站式开源 LLM 部署、推理、微调的使用教程 https://github.com/datawhalechina/self-llm LLM-Universe （动手学大模型应用开发） 指导开发者从零开始搭建自己的 LLM 应用 https://github.com/datawhalechina/llm-universe Happy-LLM （从零开始的大语言模型原理与实践） 深入 LLM 原理 + 动手复现 LLaMA2 https://github.com/datawhalechina/happy-llm 笔者一点点感受： LLM真的很奇妙，它让计算机用计算的方式能够生成人类语言，明明只是0101，却通过各种参数使得人类的语言符号被学习、理解、生成。虽然机器不像人类那样有脑子🧠，但是也感觉到很奇妙，似乎发现了更广阔神秘的天地。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm1/:4:0","tags":["LLM"],"title":"Happy_LLM_01 NLP前言","uri":"/learn-documentation-happy-llm1/"},{"categories":["learn-notes"],"content":"bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器.","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"Task02：第一章 NLP 基础概念 （这是笔者自己的学习记录，仅供参考，原始学习链接，愿 LLM 越来越好❤） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:0:0","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"NLP 自然语言处理 顾名思义就是处理人类语言的，把人类的语言用某种方式让计算机理解，理解之后便能进行一些处理和上操作。计算机能够用底层的0101模拟出人类的语言后，人们就开始整活，让他做事了，比如中文分词等，这里面可能会涉及语义、语境、情感、文化等。所以一直以来，大家都在不断改进算法方法来使计算机更能理解人类语言，从而达到对人类的语言数据进行一些处理。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:1:0","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"NLP 的发展历程 基于 规则 进行处理：主要做翻译任务，给计算机字典和词序规则，让他查。 基于 统计 概率进行处理：统计模型，机器学习 基于 深度学习 模型：主要的比如 RNN、LSTM、Word2Vec、BERT、Transformer ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:2:0","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"NLP 的一些任务（人类都让它整了哪些活儿） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:0","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"1、中文分词（Chinese Word Segmentation） 基础任务。中文不像英文空格是一个词，中文有字和词的概念。所以处理中文文本，首先就是要进行分词，分的好意味着理解的好和正确，对后续的整活很关键。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:1","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"2、子词切分（Subword Segmentation） 是常见的文本预处理技术。像英文就是再把词分成前后缀，以后遇到没见过的可能有举一反三的能力。 例如：unhappiness = un-happy-ness 常见的方法：Byte Pair Encoding (BPE)、WordPiece、Unigram、SentencePiece 等 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:2","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"3、词性标注（Part-of-Speech Tagging） 基础任务。就是标注词性。n、v、adj、adv 这种，对理解语义很重要。通常是用机器学习模型，从标注数据里学一学就会了。 常见的 ML 模型：隐马尔可夫模型（Hidden Markov Model，HMM）、条件随机场（Conditional Random Field，CRF） 常见的 DL 模型：RNN、LSTM ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:3","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"4、文本分类（Text Classification） 核心任务。分类任务，就是给文本你把这个话分到某个领域类别中，如教育、医疗、财经这些类。机器学习比较多，挺简单的，主要就是要选合适的特征表示和分类算法，训练的数据也很重要。但是现在深度学习变成趋势了。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:4","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"5、实体识别（Named Entity Recognition） 关键任务。就是给文本，他能从长句子里面提取各种类别的信息，这就是实体，比如里面说的人、地名、时间这种。比文本分类更细，文本可能看见关键词就分类了。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:5","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"6、关系抽取（Relation Extraction） 关键任务。在实体识别的基础上，能够理解实体之间的关系，比如从属、因果等。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:6","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"7、文本摘要（Text Summarization） 重要任务。把长文本进行内容概括， 抽取式摘要：（优点）摘要内容全是来自原文，准确性比较高。（缺点）可能不通畅。 生成式摘要：对内容进行重新组合改写，更难，需要如 Seq2Seq 的模型。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:7","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"8、机器翻译（Machine Translation） 核心任务。就是不同语音之间进行翻译，重点是转化、准确、风格、文化因素。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:8","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"9、自动问答（Automatic Question Answering，QA） 高级任务。理解人给的问题并回答，这涉及了很多子任务（信息检索、文本理解、知识表示和推理等） 有 3 类： 检索式问答：就是搜索出答案。 知识库问答：基于知识库结构化内容检索出答案。 社区问答：基于用户的论坛 csdn 啥的。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:3:9","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"文本表示是什么？ 目的是用计算机能处理的方式表示自然语言，文本数据数字化。（字、词、短语、句子）表示形式可能有（向量、矩阵或其他数据结构） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:4:0","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"文本表示的发展历程 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:5:0","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"1、词向量 —— 向量空间模型 VSM 是文本表示的方法。 VSM 做什么的？ 文本变高维向量，每一维是特征项（字、词、短语），值（通过公式计算得到，如 TF 词频、TF-IDF 逆文档频率）是代表特征项的权重 = 在文本中的重要程度。 VSM 变换的优化方法？ 改进特征 表示 方法：图方法、主题方法 改进特征项 权重计算 方法：如用矩阵运算（特征值计算、奇异值分解等方法），提升效率和效果。 VSM 的问题？ 数据稀疏性 + 维数灾难 特征项选择 权重计算方法 （有个词汇表大小就是一个词的向量维数，该词的向量表示就是，词汇表中他的位置值为 1，其他 0，所以很浪费。） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:5:1","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"2、N-gram 语言模型 是基于统计条件概率的模型。基于马尔可夫假设，现在这个词的出现概率依赖前 N-1 个词（N 是正整数，具体前几个词也不一定。 N=1 是 unigram 模型、N=2 bigram、N=3 trigram，就是前两个词出现的情况下，这个词出现的概率） N-gram 做什么的？ 可以计算这个词的出现概率 或者从后往前推算概率连乘，计算整个句子的 N-gram 的优点？ 易理解简单 N-gram 的问题？ N 大时，出现推理时某些概率太低 数据稀疏性问题，就是训练语料中没出现过的词序组合，泛化能力低 忽略了远距离词之间的关系 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:5:2","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"3、Word2Vec 语言模型 是一种词嵌入技术，2013 年提出的。 两种架构： CBOW 连续词袋模型（适合小数据集）：根据目标词的上下文词的词向量计算这个词的词向量。 Skip-Gram 模型（适合大数据集）：先有目标词的词向量，预测周围词的词向量，训练过程中不断优化“吃”的向量，使它能更好地预测这些上下文词。 模型学什么的？ 用神经网络 NNLM 学词在文本中的词之间的语义关系，使得语义相似或相关的词在向量空间距离就近 Word2Vec 是做什么的？ 把语言转成低维（几百维）的密集向量，减少计算复杂度和存储 能捕捉词与词的语义关系 生成静态词向量 和 VSM 比的优势是？ W2V 基于上下文学，不是基于词典，泛化能力好点 W2V 的问题是？ 两种架构都是基于局部上下文的，长距离不行。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:5:3","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"4、ELMo Embedding from LM，是一种融入预训练方法到词向量表示的思路，生成动态词向量，实现词向量的准确表示（同一个词不同语境不同表示向量） ELMo 是怎么做的？ 先获得词向量模型（用基于 RNN 的双向 LSTM 结构在大型语料库训） 再在特定任务微调，从预训练模型提取词向量作为特征调整。 ELMo 的优势是什么？ 能捕捉词在不同语境上下文的多义 ELMo 的问题是什么？ 模型复杂度高 训练时间长 计算资源消耗大 （牛逼模型的通病吧） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm2/:5:4","tags":["LLM"],"title":"Happy_LLM_02 什么是NLP","uri":"/learn-documentation-happy-llm2/"},{"categories":["learn-notes"],"content":"bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器.","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"Task04：第二章 NLP 基础概念 Task03+04：第二章 Transformer架构 本篇是task04： 2.2 2.3 编解码器、搭建一个Transformer （这是笔者自己的学习记录，仅供参考，原始学习链接，愿 LLM 越来越好❤） Transformer架构很重要，需要分几天啃一啃。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:0:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"Transformer的定位图 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:1:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"Transformer架构图 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:2:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"Transformer论文结构图 接下来，我会按照顺序依次介绍各个部分，这样因果关系会清晰一些 顺序：tokenizer、【transformer：（embedding、encoder、decoder、线性层+softmax）】 transformer的核心：attention、encoder、decoder encoder、decoder的核心组件（3个）：层归一化（layer norm）、前馈全连接神经网络（FNN）、残差连接（residual connection） FNN：如MLP，线性层 + 非线性RELU激活函数 + 线性层 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:3:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"transformer和seq2seq的关系 transformer就是一种seq2seq模型，可以用来完成seq2seq任务 什么是seq2seq？ 序列到序列，是一种NLP经典任务，输入是文本序列，输出也是。其实所有nlp任务都可以看成是广义的seq2seq，如机器翻译、文本分类、词性标注等 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:4:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"tokenizer 分词器 分词器不在transformer架构中，他是前缀处理模块。 tokenizer作用？ 把 文本 –\u003e 切分成很多token=seq_len（有很多策略，词、子词、字符等） –\u003e index数值，变成（batchsize, seq_len, 1） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:5:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"embedding embedding有两个流程： 流程1——input embedding ：转成词向量。把前面的数值索引 根据词典向量表 变成词向量矩阵 流程2——position encoding：位置编码。根据 token在序列中的位置 得到位置编码矩阵 最后，把上面两个矩阵相加，得到这一层的输出。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:6:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"what 输入输出矩阵形状 of input embedding？ 其实就是把一维数值变成了多维的 输入形状：（batch_size, seq_len, 1） 输出形状：（batch_size, seq_len, embedding_dim） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:6:1","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"what 词典向量表？ 是一个可训练的权重矩阵，（vocab_size, embedding_dim），每一个数值index对应一个embedding_dim维的词向量。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:6:2","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"why position encoding？ 因为transformer他是并行计算，和传统的RNN、LSTM这些顺序计算的不太一样，会丢失位置信息。导致模型会认为“我喜欢你”和“你喜欢我”弄成是一样的意思。所以，要在进入编码器之前加上位置信息。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:6:3","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"how position encoding？ 位置编码有很多方式，transformer论文中用的是正余弦绝对位置编码，和输入文本的内容无关，是和序列的token数有关，以及他是奇偶位置有关，具体的计算可以参考原文。得到的位置编码矩阵的形状就是（seq_size, embedding_dim），要和词向量矩阵的维度一样才可以相加。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:6:4","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"encoder 编码器 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:7:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"encoder的整体组成 encoder：里有N个encoder layer（论文里是6个层） encoder layer：里有2个layer norm、1个attention、1个fnn 总结：先归一化，再给attention，输出后归一化，再进fnn。然后6个layer组装，再归一化。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:7:1","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"2个细节？ 层归一化收敛快一点 残差连接不让模型走偏，下一层input = 上一层output + 上一层 input ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:7:2","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"代码实现 先写encoder layer类，再组装出encoder class EncoderLayer(nn.Module): '''Encoder层''' def __init__(self, args): super().__init__() # 一个 Layer 中有两个 LayerNorm，分别在 Attention 之前和 MLP 之前 self.attention_norm = LayerNorm(args.n_embd) # Encoder 不需要掩码，传入 is_causal=False self.attention = MultiHeadAttention(args, is_causal=False) self.fnn_norm = LayerNorm(args.n_embd) self.feed_forward = MLP(args.dim, args.dim, args.dropout) def forward(self, x): # Layer Norm norm_x = self.attention_norm(x) # 自注意力 h = x + self.attention.forward(norm_x, norm_x, norm_x) # 经过前馈神经网络 out = h + self.feed_forward.forward(self.fnn_norm(h)) return out\rclass Encoder(nn.Module): '''Encoder 块''' def __init__(self, args): super(Encoder, self).__init__() # 一个 Encoder 由 N 个 Encoder Layer 组成 self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.n_layer)]) self.norm = LayerNorm(args.n_embd) def forward(self, x): \"分别通过 N 层 Encoder Layer\" for layer in self.layers: x = layer(x) return self.norm(x)\r","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:7:3","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"拓：深度神经网络中的归一化？ 有两种： 批归一化（batch norm） 层归一化（layer norm）：transformer里用的是这个 都涉及均值和方差，具体的公式先不管 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:7:4","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"decoder 解码器 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:8:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"decoder的整体组成 decoder和encoder很像，就是单层里的结构有点差异 decoder：里有N个decoder layer（论文里也是6个层） decoder layer：里有3个layer norm、2个attention（mask self attention、multi head attention）、1个fnn 总结：先归一化，经过掩码自注意，再归一化，经过多头注意力，再归一化，经过fnn。再把6个层组装，再归一化 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:8:1","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"decoder的两个注意力层 第一个注意力层是一个掩码自注意力层，即使用 Mask 的注意力计算，保证每一个 token 只能使用该 token之前的注意力分数； 第二个注意力层是一个多头注意力层，该层将使用第一个注意力层的输出作为 query，使用 Encoder 的输出作为 key和 value，来计算注意力分数。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:8:2","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"代码实现 class DecoderLayer(nn.Module): '''解码层''' def __init__(self, args): super().__init__() # 一个 Layer 中有三个 LayerNorm，分别在 Mask Attention 之前、Self Attention 之前和 MLP 之前 self.attention_norm_1 = LayerNorm(args.n_embd) # Decoder 的第一个部分是 Mask Attention，传入 is_causal=True self.mask_attention = MultiHeadAttention(args, is_causal=True) self.attention_norm_2 = LayerNorm(args.n_embd) # Decoder 的第二个部分是 类似于 Encoder 的 Attention，传入 is_causal=False self.attention = MultiHeadAttention(args, is_causal=False) self.ffn_norm = LayerNorm(args.n_embd) # 第三个部分是 MLP self.feed_forward = MLP(args.dim, args.dim, args.dropout) def forward(self, x, enc_out): # Layer Norm norm_x = self.attention_norm_1(x) # 掩码自注意力 x = x + self.mask_attention.forward(norm_x, norm_x, norm_x) # 多头注意力 norm_x = self.attention_norm_2(x) h = x + self.attention.forward(norm_x, enc_out, enc_out) # 经过前馈神经网络 out = h + self.feed_forward.forward(self.ffn_norm(h)) return out\rclass Decoder(nn.Module): '''解码器''' def __init__(self, args): super(Decoder, self).__init__() # 一个 Decoder 由 N 个 Decoder Layer 组成 self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.n_layer)]) self.norm = LayerNorm(args.n_embd) def forward(self, x, enc_out): \"Pass the input (and mask) through each layer in turn.\" for layer in self.layers: x = layer(x, enc_out) return self.norm(x)\r","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:8:3","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"末尾步骤 经过一个线性层，再经过softmax。softmax就是只取正，否则就是0 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:9:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"手搓Transformer代码 论文中写的归一化是post-norm，但是发出来的代码中用的是pre-norm，鉴于pre-norm可以让loss更稳定，便默认用pre-norm（就是在输入每个attention或者fnn前归一化） 经过 tokenizer 映射后的输出先经过 Embedding 层和 Positional Embedding层编码，然后进入 N 个 Encoder 和 N 个 Decoder（在 Transformer 原模型中，N取为6），最后经过一个线性层和一个 Softmax 层就得到了最终输出。 class Transformer(nn.Module): '''整体模型''' def __init__(self, args): super().__init__() # 必须输入词表大小和 block size assert args.vocab_size is not None assert args.block_size is not None self.args = args self.transformer = nn.ModuleDict(dict( wte = nn.Embedding(args.vocab_size, args.n_embd), wpe = PositionalEncoding(args), drop = nn.Dropout(args.dropout), encoder = Encoder(args), decoder = Decoder(args), )) # 最后的线性层，输入是 n_embd，输出是词表大小 self.lm_head = nn.Linear(args.n_embd, args.vocab_size, bias=False) # 初始化所有的权重 self.apply(self._init_weights) # 查看所有参数的数量 print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,)) '''统计所有参数的数量''' def get_num_params(self, non_embedding=False): # non_embedding: 是否统计 embedding 的参数 n_params = sum(p.numel() for p in self.parameters()) # 如果不统计 embedding 的参数，就减去 if non_embedding: n_params -= self.transformer.wte.weight.numel() return n_params '''初始化权重''' def _init_weights(self, module): # 线性层和 Embedding 层初始化为正则分布 if isinstance(module, nn.Linear): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) if module.bias is not None: torch.nn.init.zeros_(module.bias) elif isinstance(module, nn.Embedding): torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) '''前向计算函数''' def forward(self, idx, targets=None): # 输入为 idx，维度为 (batch size, sequence length, 1)；targets 为目标序列，用于计算 loss device = idx.device b, t = idx.size() assert t \u003c= self.args.block_size, f\"不能计算该序列，该序列长度为 {t}, 最大序列长度只有 {self.args.block_size}\" # 通过 self.transformer # 首先将输入 idx 通过 Embedding 层，得到维度为 (batch size, sequence length, n_embd) print(\"idx\",idx.size()) # 通过 Embedding 层 tok_emb = self.transformer.wte(idx) print(\"tok_emb\",tok_emb.size()) # 然后通过位置编码 pos_emb = self.transformer.wpe(tok_emb) # 再进行 Dropout x = self.transformer.drop(pos_emb) # 然后通过 Encoder print(\"x after wpe:\",x.size()) enc_out = self.transformer.encoder(x) print(\"enc_out:\",enc_out.size()) # 再通过 Decoder x = self.transformer.decoder(x, enc_out) print(\"x after decoder:\",x.size()) if targets is not None: # 训练阶段，如果我们给了 targets，就计算 loss # 先通过最后的 Linear 层，得到维度为 (batch size, sequence length, vocab size) logits = self.lm_head(x) # 再跟 targets 计算交叉熵 loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1) else: # 推理阶段，我们只需要 logits，loss 为 None # 取 -1 是只取序列中的最后一个作为输出 logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim loss = None return logits, loss\r注意，上述代码除去搭建了整个 Transformer 结构外，还额外实现了三个函数： get_num_params：用于统计模型的参数量 _init_weights：用于对模型所有参数进行随机初始化 forward：前向计算函数 强调：本篇中所有代码都是原文链接中提供的，不是主包自己贡献的！！感谢开源大佬 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm4/:10:0","tags":["LLM"],"title":"Happy_LLM_04 编解码器、手搓Transformer","uri":"/learn-documentation-happy-llm4/"},{"categories":["learn-notes"],"content":"Discover what the Hugo - LoveIt theme is all about and the core-concepts behind it.","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"Discover what the Hugo - LoveIt theme is all about and the core-concepts behind it. Task03-05：第二章 Transformer架构 本篇是task03： 2.1 注意力机制 （这是笔者自己的学习记录，仅供参考，原始学习链接，愿 LLM 越来越好❤） Transformer架构很重要，需要分几天啃一啃。 在NLP中的核心基础任务文本表示，从用统计方法得到向量进入用神经网络方法。而这个神经网络NN（Neural Network）确实从CV计算机视觉发展来的。 所以我们应该先了解一下CV中神经网络的核心架构。 一、CV中NN的核心架构（共3种） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:0:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"1、FNN（全连接 Feedforward NN）： 连接方式：每一层的每个神经元都和上下的每个神经元连接。 参数量：全连接层的参数量 = 输入维度 × 输出维度，6层的网络，要计算6-1次相加 特点：简单但是参数量巨大 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:1:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"2、CNN（卷积 Convolutional NN）： 连接方式：卷积核 参数量：3x3（卷积核）x 输入通道数 x 输出通道数 特点：参数量远小于FNN的，进行特征提取和学习 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:2:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"3、RNN（循环 Recycle NN）： 有循环，输出作为输入 二、NN在NLP的使用发展 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:3:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"1、RNN、LSTM架构 以前用的比较多（LSTM是RNN的衍生，如ELMo文本表示模型用的双向LSTM）NLP处理的是文本序列，用这种架构效果比其他2种好。 这种架构的优点？ 能捕捉时序信息、适合序列生成 架构的问题？ 2个 一个是串行不能并行计算，时间久。 另一个是RNN长距离关系很难捕捉，且要把整个序列读到内存，对序列长度有限制 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:4:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"2、Transformer架构 现在火起来的，架构中的核心是Attention注意力机制（这个机制常被融到RNN中，现在被单拎出来做成新的NN架构，并用在NLP作为LLM的核心架构了）也是深度学习里最核心的架构。 总结：架构核心是CV领域RNN的Attention机制，是神经网络、深度学习的架构，现用在NLP的LLM中 Attention注意力机制的思想是什么？ cv领域的思想是看图片只要关注重点部分（比如看海边画你不用每个细节注意到，你只要看天颜色美你就觉得画好看）； nlp领域的思想是语言只要关注重点token进行计算（人的语言理解也是，听别人说话可能你听些词就能自动脑补别人全部意思了） 三、注意力机制公式推导 attention的核心计算公式是什么？ $$ attention(Q,K,V) = softmax（ \\frac{QK^T}{\\sqrt{d _k}}）V. $$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:5:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"1、核心变量 Q（query查询值）、K（key键）、V（value值） 【大写为矩阵，小写为向量。tt = tokens target ，ts = tokens source】 q词向量 =（1 x dk） Q矩阵 = （目标序列token数 ntt x dk） K矩阵 = （源序列token数 nts x dk） V矩阵 = （源序列token数 nts x dv） 备注：K和V都来自同一个输入序列，行数是一样的。k和v都来自同一个token，但是数值却不同。（因为token进行embedding后会有一个x向量，会分别乘不同的权重矩阵进行线性变换。k = xWk，v=xWv。） （多头注意力中，当dmodel=512，head=8时） $$ d_k = d_v= \\frac{d_{model} }{h} = \\frac{512}{8} = 64$$ （自注意力中，QKV来自同一个序列，每个token对应一行） $$ n_{tt} = n_{ts}$$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:6:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"2、公式拆解 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:7:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"拆解式子1——得到权重： 单个token（“fruit”）和序列每个token（“apple”、“banana”，“car”）的相似度【点积】 $$ x = qK^{\\mathsf{T}} ~~维度(1 \\times n_{ts} ) $$维度变化如下：$$ q \\in \\mathbb{R}^{1 \\times d_k}, \\quad K \\in \\mathbb{R}^{n_{ts} \\times d_k} $$$$ (1 \\times d_k) \\cdot (n_{tt} \\times d_k)^{\\mathsf{T}} = (1 \\times d_k) \\cdot (d_k \\times n_{ts}) = (1 \\times n_{ts})$$结果值含义如下： $$ q_{fruit} = \\begin{bmatrix} 1.0 \u0026 0.5 \u0026 -0.3 \u0026 0.8 \\end{bmatrix} ,\\quad$$$$ K = \\begin{bmatrix} k_{apple}:0.9 \u0026 0.4 \u0026 -0.2 \u0026 0.7 \\ k_{banana}:0.8 \u0026 0.6 \u0026 -0.1 \u0026 -0.6 \\ k_{car}:-0.5 \u0026 0.2 \u0026 0.9 \u0026 -0.4 \\end{bmatrix} $$$$ x = \\begin{bmatrix} 1.72 \u0026 1.61 \u0026 -0.99 \\end{bmatrix} .\\quad$$q和K矩阵中每个向量进行点积， $x$向量的每个数值对应fruit和序列中每个token的相关性=注意力分数=权重 同理， $$ X = QK^T ~~维度(n_{tt} \\times n_{ts} )$$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:7:1","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"拆解式子2——得到对其他词的关注度： softmax 将 $x$ 或者说$X$转成权重值加起来等于1（统一一下）。 (当$d_k$比较大时，这里需要对$x$进行一个放缩，即除以$\\sqrt{d_k}$再用$softmax$) 目的：否则经过函数后不同值得差异很大，容易影响梯度稳定性 $$softmax(x){i} = \\frac{e^{x{i}}}{Z}= \\frac{e^{x_{i}}}{\\sum_{j=0}^{n_{ts}-1}{e^{x_{j}}}}$$结果值如下： （不放缩时）$$ x = \\begin{bmatrix} 1.72 \u0026 1.61 \u0026 -0.99 \\end{bmatrix} .\\quad$$$$ e^{1.72} \\approx 5.58, \\quad e^{1.61} \\approx 5.00, \\quad e^{-0.99} \\approx 0.37 $$$$Z=5.58+5.00+0.37=10.95$$$$ x_{scaled} =\\begin{bmatrix} \\frac{0.51}{10.95} \u0026 \\frac{0.46}{10.95} \u0026 \\frac{0.03}{10.95} \\end{bmatrix}= \\begin{bmatrix} 0.51 \u0026 0.46 \u0026 0.03 \\end{bmatrix} .\\quad$$ （放缩时，假设$d_k$=4很大，这里是除以2） $$ x = \\begin{bmatrix} 0.86 \u0026 0.805 \u0026 -0.495 \\end{bmatrix} .\\quad$$$$ x_{scaled} =\\begin{bmatrix} \\frac{2.364}{5.211} \u0026 \\frac{2.237}{5.211} \u0026 \\frac{0.610}{5.211} \\end{bmatrix}= \\begin{bmatrix} 0.454 \u0026 0.429 \u0026 0.117 \\end{bmatrix} .\\quad$$fruit 对 apple 的关注度45.4%，对 banana 的关注度42.9%，对 car 的关注度11.7%【这是fruit对别人的】 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:7:2","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"拆解式子3——得到q本身的表示： K和V可以一样，因为来自同一个序列。具体是否相同和第一步embedding的线性变换的权重矩阵$W^k$、$W^v$有关。 权重和V矩阵的各个向量【加权求和】 $$attention= x_{scaled}\\cdot V$$ （这里假设K、V不一样）$$ V = \\begin{bmatrix} v_{apple}:1.2 \u0026 0.3 \u0026 0.5 \u0026 0.9 \\ v_{banana}:1.0 \u0026 0.4 \u0026 0.6 \u0026 0.8 \\ v_{car}:0.2 \u0026 0.9 \u0026 1.1 \u0026 0.1 \\end{bmatrix} $$$$ x_{scaled} = \\begin{bmatrix} 0.454 \u0026 0.429 \u0026 0.117 \\end{bmatrix} .\\quad$$ 结果值如下： $$ attention_{fruit} = \\begin{bmatrix} 0.9972 \u0026 0.4131 \u0026 0.6131\u00260.7635 \\end{bmatrix}$$每一维都是fruit在不同语义方面特征的示意程度【这是fruit自己的性质，用来描述fruit的】，可以观察到结果和apple、banana的kv向量各维度数值都比较接近。 你可以把 Attention输出向量想象成： “我看了上下文中所有相关的信息（apple、banana、car），根据它们和‘fruit’的关系，我总结出fruit这四个方面的重点信息。” ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:7:3","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"3、代码实现 '''注意力计算函数''' def attention(query, key, value, dropout=None): ''' args: query: 查询值矩阵 key: 键值矩阵 value: 真值矩阵 ''' # 获取键向量的维度，键向量的维度和值向量的维度相同 d_k = query.size(-1) # 计算Q与K的内积并除以根号dk # transpose——相当于转置 scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # Softmax p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) # 采样 # 根据计算结果对value进行加权求和 return torch.matmul(p_attn, value), p_attn\r四、注意力机制的使用及衍生 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:8:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"1、注意力 Attention： 在 Decoder 中，Q来自 Decoder 的输入，KV来自 Encoder 的输出。目的是拟合编码信息和历史信息之间的关系，综合预测未来。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:9:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"2、自注意力 Self-Attention： 在 Encoder 中，采用自注意力机制 Self-Attention（Attention变种），QKV是同一个输入，分别经过3个参数矩阵得到。目的是拟合输入语句中每一个token对其他所有token的依赖关系。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:10:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"3、 掩码自注意力 Mask Self-Attention 用掩码遮蔽某些位置的token，不让模型学习的时候学未来的。类似n-gram，不过他是串行，达咩。所以一次性输入下面的这个就可以并行计算，得到语言模型。掩码矩阵就是和文本序列token数等长的上三角矩阵。 ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"mask长什么样 （当 n = 4时，代码的-inf就是负无穷） $$ Mask = \\begin{bmatrix} 0 \u0026 -\\infty \u0026 -\\infty \u0026 -\\infty \\ 0 \u0026 0 \u0026 -\\infty \u0026 -\\infty \\ 0 \u0026 0 \u0026 0 \u0026 -\\infty \\ 0 \u0026 0 \u0026 0 \u0026 0 \\end{bmatrix} $$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:1","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"代码实现mask矩阵 # 创建一个上三角矩阵，用于遮蔽未来信息。 # 先通过 full 函数创建一个 1 * seq_len * seq_len 的矩阵 mask = torch.full((1, args.max_seq_len, args.max_seq_len), float(\"-inf\")) # triu 函数的功能是创建一个上三角矩阵 mask = torch.triu(mask, diagonal=1)\r","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:2","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"mask怎么用？ 是在得出注意力分数之后，把X和Mask相加，-inf就会覆盖X的原值，经过softmax后就变成0，注意力遮蔽了。 # 此处的 scores 为计算得到的注意力分数，mask 为上文生成的掩码矩阵 scores = scores + mask[:, :seqlen, :seqlen] scores = F.softmax(scores.float(), dim=-1).type_as(xq)\r","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:3","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"什么是张量 Tensor？ 矩阵： $$ A = \\begin{bmatrix} 1 \u0026 -2 \u0026 3 \\ 0 \u0026 4 \u0026 -1 \\end{bmatrix}\\in \\mathbb{R}^{2 \\times 3} $$三维张量：$$ \\mathcal{T} = \\left{ \\begin{bmatrix} 1 \u0026 2 \u0026 3 \\ 4 \u0026 5 \u0026 6 \\end{bmatrix}, \\begin{bmatrix} 7 \u0026 8 \u0026 9 \\ 10 \u0026 11 \u0026 12 \\end{bmatrix} \\right} \\in \\mathbb{R}^{2 \\times 2 \\times 3} $$四维张量：$$ \\mathcal{T} = \\left{ \\begin{array}{c} \\text{样本 1: } \\left[ \\begin{array}{c} \\text{通道 1: } 4 \\times 5 \\ \\text{通道 2: } 4 \\times 5 \\ \\text{通道 3: } 4 \\times 5 \\end{array} \\right], \\[1ex] \\text{样本 2: } \\left[ \\begin{array}{c} \\text{通道 1: } 4 \\times 5 \\ \\text{通道 2: } 4 \\times 5 \\ \\text{通道 3: } 4 \\times 5 \\end{array} \\right] \\end{array} \\right} \\in \\mathbb{R}^{2 \\times 3 \\times 4 \\times 5} $$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:4","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"维度和形状的区别？ 维度是数值，1、2、3维；形状是元组（2 x 3） ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:5","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"什么是广播机制 Broadcasting？ 就是在张量（三维及以上数组）运算的时候，能像史莱姆一样自动拓展维度成目标张量的形状。 掩码矩阵的shape为： $$（1，seq_len，seq_len）$$注意力分数的shape为：$$（batch_size，n_heads，seq_len，seq_len）$$广播后的掩码矩阵的shape为：$$（batch_size，n_heads，seq_len，seq_len）$$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:11:6","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"4、多头注意力： 就是对同一个序列进行多次注意力计算，再拼接结果，每次可以学到不同的关系，从而让对token的表示更加深入。 n个头就是计算n次的注意力 公式：$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O $$ 其中： $$ \\text{head}_i = \\text{Attention}(Q W_i^Q, ; K W_i^K, ; V W_i^V) $$ ","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:12:0","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["learn-notes"],"content":"代码实现 import torch.nn as nn import torch '''多头自注意力计算模块''' class MultiHeadAttention(nn.Module): def __init__(self, args: ModelArgs, is_causal=False): # 构造函数 # args: 配置对象 super().__init__() # 隐藏层维度必须是头数的整数倍，因为后面我们会将输入拆成头数个矩阵 assert args.dim % args.n_heads == 0 # 模型并行处理大小，默认为1。 model_parallel_size = 1 # 本地计算头数，等于总头数除以模型并行处理大小。 self.n_local_heads = args.n_heads // model_parallel_size # 每个头的维度，等于模型维度除以头的总数。 self.head_dim = args.dim // args.n_heads # Wq, Wk, Wv 参数矩阵，每个参数矩阵为 n_embd x n_embd # 这里通过三个组合矩阵来代替了n个参数矩阵的组合，其逻辑在于矩阵内积再拼接其实等同于拼接矩阵再内积， # 不理解的读者可以自行模拟一下，每一个线性层其实相当于n个参数矩阵的拼接 self.wq = nn.Linear(args.dim, self.n_local_heads * self.head_dim, bias=False) self.wk = nn.Linear(args.dim, self.n_local_heads * self.head_dim, bias=False) self.wv = nn.Linear(args.dim, self.n_local_heads * self.head_dim, bias=False) # 输出权重矩阵，维度为 dim x n_embd（head_dim = n_embeds / n_heads） self.wo = nn.Linear(self.n_local_heads * self.head_dim, args.dim, bias=False) # 注意力的 dropout self.attn_dropout = nn.Dropout(args.dropout) # 残差连接的 dropout self.resid_dropout = nn.Dropout(args.dropout) # 创建一个上三角矩阵，用于遮蔽未来信息 # 注意，因为是多头注意力，Mask 矩阵比之前我们定义的多一个维度 if is_causal: mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\")) mask = torch.triu(mask, diagonal=1) # 注册为模型的缓冲区 self.register_buffer(\"mask\", mask) def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor): # 获取批次大小和序列长度，[batch_size, seq_len, dim] bsz, seqlen, _ = q.shape # 计算查询（Q）、键（K）、值（V）,输入通过参数矩阵层，维度为 (B, T, n_embed) x (n_embed, n_embed) -\u003e (B, T, n_embed) xq, xk, xv = self.wq(q), self.wk(k), self.wv(v) # 将 Q、K、V 拆分成多头，维度为 (B, T, n_head, C // n_head)，然后交换维度，变成 (B, n_head, T, C // n_head) # 因为在注意力计算中我们是取了后两个维度参与计算 # 为什么要先按B*T*n_head*C//n_head展开再互换1、2维度而不是直接按注意力输入展开，是因为view的展开方式是直接把输入全部排开， # 然后按要求构造，可以发现只有上述操作能够实现我们将每个头对应部分取出来的目标 xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim) xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim) xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim) xq = xq.transpose(1, 2) xk = xk.transpose(1, 2) xv = xv.transpose(1, 2) # 注意力计算 # 计算 QK^T / sqrt(d_k)，维度为 (B, nh, T, hs) x (B, nh, hs, T) -\u003e (B, nh, T, T) scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim) # 掩码自注意力必须有注意力掩码 if self.is_causal: assert hasattr(self, 'mask') # 这里截取到序列长度，因为有些序列可能比 max_seq_len 短 scores = scores + self.mask[:, :, :seqlen, :seqlen] # 计算 softmax，维度为 (B, nh, T, T) scores = F.softmax(scores.float(), dim=-1).type_as(xq) # 做 Dropout scores = self.attn_dropout(scores) # V * Score，维度为(B, nh, T, T) x (B, nh, T, hs) -\u003e (B, nh, T, hs) output = torch.matmul(scores, xv) # 恢复时间维度并合并头。 # 将多头的结果拼接起来, 先交换维度为 (B, T, n_head, C // n_head)，再拼接成 (B, T, n_head * C // n_head) # contiguous 函数用于重新开辟一块新内存存储，因为Pytorch设置先transpose再view会报错， # 因为view直接基于底层存储得到，然而transpose并不会改变底层存储，因此需要额外存储 output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # 最终投影回残差流。 output = self.wo(output) output = self.resid_dropout(output) return output","date":"2025-08-26","objectID":"/learn-documentation-happy-llm3/:12:1","tags":["LLM","Transformer"],"title":"Happy_LLM_03 Transformer的注意力机制","uri":"/learn-documentation-happy-llm3/"},{"categories":["documentation"],"content":"了解如何在 LoveIt 主题中快速, 直观地创建和组织内容.","date":"2020-03-05","objectID":"/theme-documentation-content/","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"了解如何在 LoveIt 主题中快速, 直观地创建和组织内容. ","date":"2020-03-05","objectID":"/theme-documentation-content/:0:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"1 内容组织 以下是一些方便你清晰管理和生成文章的目录结构建议: 保持博客文章存放在 content/posts 目录, 例如: content/posts/我的第一篇文章.md 保持简单的静态页面存放在 content 目录, 例如: content/about.md 本地资源组织 本地资源引用\r有三种方法来引用图片和音乐等本地资源: 使用页面包中的页面资源. 你可以使用适用于 Resources.GetMatch 的值或者直接使用相对于当前页面目录的文件路径来引用页面资源. 将本地资源放在 assets 目录中, 默认路径是 /assets. 引用资源的文件路径是相对于 assets 目录的. 将本地资源放在 static 目录中, 默认路径是 /static. 引用资源的文件路径是相对于 static 目录的. 引用的优先级符合以上的顺序. 在这个主题中的很多地方可以使用上面的本地资源引用, 例如 链接, 图片, image shortcode, music shortcode 和前置参数中的部分参数. 页面资源或者 assets 目录中的图片处理会在未来的版本中得到支持. 非常酷的功能! ","date":"2020-03-05","objectID":"/theme-documentation-content/:1:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"2 前置参数 Hugo 允许你在文章内容前面添加 yaml, toml 或者 json 格式的前置参数. 注意\r不是所有的以下前置参数都必须在你的每篇文章中设置. 只有在文章的参数和你的 网站设置 中的 page 部分不一致时才有必要这么做.\r这是一个前置参数例子: --- title: \"我的第一篇文章\" subtitle: \"\" date: 2020-03-04T15:58:26+08:00 lastmod: 2020-03-04T15:58:26+08:00 draft: true author: \"\" authorLink: \"\" description: \"\" license: \"\" images: [] tags: [] categories: [] featuredImage: \"\" featuredImagePreview: \"\" hiddenFromHomePage: false hiddenFromSearch: false twemoji: false lightgallery: true ruby: true fraction: true fontawesome: true linkToMarkdown: true rssFullText: false toc: enable: true auto: true code: copy: true maxShownLines: 50 math: enable: false # ... mapbox: # ... share: enable: true # ... comment: enable: true # ... library: css: # someCSS = \"some.css\" # 位于 \"assets/\" # 或者 # someCSS = \"https://cdn.example.com/some.css\" js: # someJS = \"some.js\" # 位于 \"assets/\" # 或者 # someJS = \"https://cdn.example.com/some.js\" seo: images: [] # ... ---\rtitle: 文章标题. subtitle: 文章副标题. date: 这篇文章创建的日期时间. 它通常是从文章的前置参数中的 date 字段获取的, 但是也可以在 网站配置 中设置. lastmod: 上次修改内容的日期时间. draft: 如果设为 true, 除非 hugo 命令使用了 --buildDrafts/-D 参数, 这篇文章不会被渲染. author: 文章作者. authorLink: 文章作者的链接. description: 文章内容的描述. license: 这篇文章特殊的许可. images: 页面图片, 用于 Open Graph 和 Twitter Cards. tags: 文章的标签. categories: 文章所属的类别. featuredImage: 文章的特色图片. featuredImagePreview: 用在主页预览的文章特色图片. hiddenFromHomePage: 如果设为 true, 这篇文章将不会显示在主页上. hiddenFromSearch: 如果设为 true, 这篇文章将不会显示在搜索结果中. twemoji: 如果设为 true, 这篇文章会使用 twemoji. lightgallery: 如果设为 true, 文章中的图片将可以按照画廊形式呈现. ruby: 如果设为 true, 这篇文章会使用 上标注释扩展语法. fraction: 如果设为 true, 这篇文章会使用 分数扩展语法. fontawesome: 如果设为 true, 这篇文章会使用 Font Awesome 扩展语法. linkToMarkdown: 如果设为 true, 内容的页脚将显示指向原始 Markdown 文件的链接. rssFullText: 如果设为 true, 在 RSS 中将会显示全文内容. toc: 和 网站配置 中的 params.page.toc 部分相同. code: 和 网站配置 中的 params.page.code 部分相同. math: 和 网站配置 中的 params.page.math 部分相同. mapbox: 和 网站配置 中的 params.page.mapbox 部分相同. share: 和 网站配置 中的 params.page.share 部分相同. comment: 和 网站配置 中的 params.page.comment 部分相同. library: 和 网站配置 中的 params.page.library 部分相同. seo: 和 网站配置 中的 params.page.seo 部分相同. 技巧\rfeaturedImage 和 featuredImagePreview 支持本地资源引用的完整用法. 如果带有在前置参数中设置了 name: featured-image 或 name: featured-image-preview 属性的页面资源, 没有必要在设置 featuredImage 或 featuredImagePreview: resources: - name: featured-image src: featured-image.jpg - name: featured-image-preview src: featured-image-preview.jpg\r","date":"2020-03-05","objectID":"/theme-documentation-content/:2:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"3 内容摘要 LoveIt 主题使用内容摘要在主页中显示大致文章信息。Hugo 支持生成文章的摘要. 文章摘要预览\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"自动摘要拆分 默认情况下, Hugo 自动将内容的前 70 个单词作为摘要. 你可以通过在 网站配置 中设置 summaryLength 来自定义摘要长度. 如果您要使用 CJK中文/日语/韩语 语言创建内容, 并且想使用 Hugo 的自动摘要拆分功能，请在 网站配置 中将 hasCJKLanguage 设置为 true. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:1","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"手动摘要拆分 另外, 你也可以添加 \u003c!--more--\u003e 摘要分割符来拆分文章生成摘要. 摘要分隔符之前的内容将用作该文章的摘要. 注意\r请小心输入\u003c!--more--\u003e ; 即全部为小写且没有空格.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:2","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"前置参数摘要 你可能希望摘要不是文章开头的文字. 在这种情况下, 你可以在文章前置参数的 summary 变量中设置单独的摘要. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:3","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"使用文章描述作为摘要 你可能希望将文章前置参数中的 description 变量的内容作为摘要. 你仍然需要在文章开头添加 \u003c!--more--\u003e 摘要分割符. 将摘要分隔符之前的内容保留为空. 然后 LoveIt 主题会将你的文章描述作为摘要. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:4","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"摘要选择的优先级顺序 由于可以通过多种方式指定摘要, 因此了解顺序很有用. 如下: 如果文章中有 \u003c!--more--\u003e 摘要分隔符, 但分隔符之前没有内容, 则使用描述作为摘要. 如果文章中有 \u003c!--more--\u003e 摘要分隔符, 则将按照手动摘要拆分的方法获得摘要. 如果文章前置参数中有摘要变量, 那么将以该值作为摘要. 按照自动摘要拆分方法. 注意\r不建议在摘要内容中包含富文本块元素, 这会导致渲染错误. 例如代码块, 图片, 表格等.\r","date":"2020-03-05","objectID":"/theme-documentation-content/:3:5","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"4 Markdown 基本语法 这部分内容在 Markdown 基本语法页面 中介绍. ","date":"2020-03-05","objectID":"/theme-documentation-content/:4:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"5 Markdown 扩展语法 LoveIt 主题提供了一些扩展的语法便于你撰写文章. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:0","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"图表 GoAT 图表 (ASCII) 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 Mermaid 图表 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:1","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Emoji 支持 这部分内容在 Emoji 支持页面 中介绍. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:2","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"数学公式 LoveIt 基于 $\\KaTeX$ 提供数学公式的支持. 在你的 网站配置 中的 [params.math] 下面设置属性 enable = true, 并在文章的前置参数中设置属性 math: true来启用数学公式的自动渲染. $\\KaTeX$ 根据 特定的分隔符 来自动渲染公式. 技巧\r有一份 $\\KaTeX$ 中支持的 $\\TeX$ 函数 清单.\r注意\r由于 Hugo 在渲染 Markdown 文档时会根据 _/*/\u003e\u003e 之类的语法生成 HTML 文档, 并且有些转义字符形式的文本内容 (如 \\(/\\)/\\[/\\]/\\\\) 会自动进行转义处理, 因此需要对这些地方进行额外的转义字符表达来实现自动渲染: _ -\u003e \\_ * -\u003e \\* \u003e\u003e -\u003e \\\u003e\u003e \\( -\u003e \\\\( \\) -\u003e \\\\) \\[ -\u003e \\\\[ \\] -\u003e \\\\] \\\\ -\u003e \\\\\\\\ LoveIt 主题支持 raw shortcode 以避免这些转义字符, 它可以帮助您编写原始数学公式内容. 一个 raw 示例: 行内公式: {{\u003c raw \u003e}}\\(\\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots\\){{\u003c /raw \u003e}} 公式块: {{\u003c raw \u003e}} \\[ a=b+c \\\\ d+e=f \\] {{\u003c /raw \u003e}}\r呈现的输出效果如下: 行内公式: 公式块: 行内公式 默认的行内公式分割符有: $ ... $ \\( ... \\) (转义的: \\\\( ... \\\\)) 例如: $c = \\pm\\sqrt{a^2 + b^2}$ 和 \\\\(f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi\\\\)\r呈现的输出效果如下: $c = \\pm\\sqrt{a^2 + b^2}$ 和 \\(f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi\\) 公式块 默认的公式块分割符有: $$ ... $$ \\[ ... \\] (转义的: \\\\[ ... \\\\]) \\begin{equation} ... \\end{equation} (不编号的: \\begin{equation*} ... \\end{equation*}) \\begin{align} ... \\end{align} (不编号的: \\begin{align*} ... \\end{align*}) \\begin{alignat} ... \\end{alignat} (不编号的: \\begin{alignat*} ... \\end{alignat*}) \\begin{gather} ... \\end{gather} (不编号的: \\begin{gather*} ... \\end{gather*}) \\begin{CD} ... \\end{CD} 例如: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\\\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\\\] \\begin{equation*} \\rho \\frac{\\mathrm{D} \\mathbf{v}}{\\mathrm{D} t}=\\nabla \\cdot \\mathbb{P}+\\rho \\mathbf{f} \\end{equation*} \\begin{equation} \\mathbf{E}=\\sum_{i} \\mathbf{E}\\_{i}=\\mathbf{E}\\_{1}+\\mathbf{E}\\_{2}+\\mathbf{E}_{3}+\\cdots \\end{equation} \\begin{align} a\u0026=b+c \\\\\\\\ d+e\u0026=f \\end{align} \\begin{alignat}{2} 10\u0026x+\u00263\u0026y = 2 \\\\\\\\ 3\u0026x+\u002613\u0026y = 4 \\end{alignat} \\begin{gather} a=b \\\\\\\\ e=b+c \\end{gather} \\begin{CD} A @\u003ea\\\u003e\u003e B \\\\\\\\ @VbVV @AAcA \\\\\\\\ C @= D \\end{CD}\r呈现的输出效果如下: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\] \\begin{equation*} \\rho \\frac{\\mathrm{D} \\mathbf{v}}{\\mathrm{D} t}=\\nabla \\cdot \\mathbb{P}+\\rho \\mathbf{f} \\end{equation*} \\begin{equation} \\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots \\end{equation} \\begin{align} a\u0026=b+c \\\\ d+e\u0026=f \\end{align} \\begin{alignat}{2} 10\u0026x+\u00263\u0026y = 2 \\\\ 3\u0026x+\u002613\u0026y = 4 \\end{alignat} \\begin{gather} a=b \\\\ e=b+c \\end{gather} \\begin{CD} A @\u003ea\u003e\u003e B \\\\ @VbVV @AAcA \\\\ C @= D \\end{CD} 技巧\r你可以在 网站配置 中自定义行内公式和公式块的分割符.\rCopy-tex Copy-tex 是一个 $\\KaTeX$ 的插件. 通过这个扩展, 在选择并复制 $\\KaTeX$ 渲染的公式时, 会将其 $\\LaTeX$ 源代码复制到剪贴板. 在你的 网站配置 中的 [params.math] 下面设置属性 copyTex = true 来启用 Copy-tex. 选择并复制上一节中渲染的公式, 可以发现复制的内容为 $\\LaTeX$ 源代码. mhchem mhchem 是一个 $\\KaTeX$ 的插件. 通过这个扩展, 你可以在文章中轻松编写漂亮的化学方程式. 在你的 网站配置 中的 [params.math] 下面设置属性 mhchem = true 来启用 mhchem. $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$\r呈现的输出效果如下: $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:3","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"字符注音或者注释 LoveIt 主题支持一种 字符注音或者注释 Markdown 扩展语法: [Hugo]^(一个开源的静态网站生成工具)\r呈现的输出效果如下: Hugo一个开源的静态网站生成工具 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:4","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"分数 LoveIt 主题支持一种 分数 Markdown 扩展语法: [浅色]/[深色] [99]/[100]\r呈现的输出效果如下: 浅色/深色 90/100 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:5","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Font Awesome LoveIt 主题使用 Font Awesome 作为图标库. 你同样可以在文章中轻松使用这些图标. 从 Font Awesome 网站 上获取所需的图标 class. 去露营啦! :(fas fa-campground fa-fw): 很快就回来. 真开心! :(far fa-grin-tears):\r呈现的输出效果如下: 去露营啦!  很快就回来. 真开心! ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:6","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"转义字符 在某些特殊情况下 (编写这个主题文档时 ), 你的文章内容会与 Markdown 的基本或者扩展语法冲突, 并且无法避免. 转义字符语法可以帮助你渲染出想要的内容: {?X} -\u003e X\r例如, 两个 : 会启用 emoji 语法. 但有时候这不是你想要的结果. 可以像这样使用转义字符语法: {?:}joy:\r呈现的输出效果如下: :joy: 而不是 😂 技巧\r这个方法可以间接解决一个还未解决的 Hugo 的 issue.\r另一个例子是: [link{?]}(#escape-character)\r呈现的输出效果如下: [link](#escape-character) 而不是 link. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:7","tags":["content","Markdown"],"title":"主题文档 - 内容","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁.","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁. Hugo 使用 Markdown 为其简单的内容格式. 但是, Markdown 在很多方面都无法很好地支持. 你可以使用纯 HTML 来扩展可能性. 但这恰好是一个坏主意. 大家使用 Markdown, 正是因为它即使不经过渲染也可以轻松阅读. 应该尽可能避免使用 HTML 以保持内容简洁. 为了避免这种限制, Hugo 创建了 shortcodes. shortcode 是一个简单代码段, 可以生成合理的 HTML 代码, 并且符合 Markdown 的设计哲学. Hugo 附带了一组预定义的 shortcodes, 它们实现了一些非常常见的用法. 提供这些 shortcodes 是为了方便保持你的 Markdown 内容简洁. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:0:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"1 figure figure 的文档 一个 figure 示例: {{\u003c figure src=\"/images/zion-national-park.jpg\" alt=\"A photograph of Zion National Park\" link=\"https://www.nps.gov/zion/index.htm\" caption=\"Zion National Park\" class=\"ma0 w-75\" \u003e}}\r呈现的输出效果如下: Zion National Park 输出的 HTML 看起来像这样: \u003cfigure class=\"ma0 w-75\"\u003e \u003ca href=\"https://www.nps.gov/zion/index.htm\"\u003e \u003cimg src=\"/images/zion-national-park.jpg\" alt=\"A photograph of Zion National Park\"\u003e \u003c/a\u003e \u003cfigcaption\u003e \u003cp\u003eZion National Park\u003c/p\u003e \u003c/figcaption\u003e \u003c/figure\u003e\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:1:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"2 gist gist 的文档 一个 gist 示例: {{\u003c gist spf13 7896402 \u003e}}\r呈现的输出效果如下: 输出的 HTML 看起来像这样: \u003cscript type=\"application/javascript\" src=\"https://gist.github.com/spf13/7896402.js\"\u003e\u003c/script\u003e\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:2:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"3 highlight highlight 的文档 一个 highlight 示例: {{\u003c highlight go \u003e}} package main import \"fmt\" func main() { fmt.Println(\"Hello, 世界\") } {{\u003c /highlight \u003e}}\r呈现的输出效果如下: package main import \"fmt\" func main() { fmt.Println(\"Hello, 世界\") }\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:3:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"4 instagram instagram 的文档 一个 instagram 示例: {{\u003c instagram CxOWiQNP2MO \u003e}}\r呈现的输出效果如下: View this post on Instagram\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:4:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"5 param param 的文档 一个 param 示例: {{\u003c param description \u003e}}\r呈现的输出效果如下: Hugo 提供了多个内置的 Shortcodes, 以方便作者保持 Markdown 内容的整洁. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:5:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"6 x x 的文档 一个 x 示例: {{\u003c x user=GoHugoIO id=917359331535966209 \u003e}}\r呈现的输出效果如下: [X embed disabled]\r","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:6:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"7 vimeo vimeo 的文档 一个 vimeo 示例: {{\u003c vimeo 146022717 \u003e}}\r呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:7:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"8 youtube youtube 的文档 一个 youtube 示例: {{\u003c youtube w7Ft2ymGmfc \u003e}}\r呈现的输出效果如下: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:8:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"9 ref ref 的文档 ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:9:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"10 relref relref 的文档 ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:10:0","tags":["shortcodes"],"title":"主题文档 - 内置 Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"LoveIt 主题在 Hugo 内置的 shortcode 的基础上提供多个扩展的 shortcode.","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"LoveIt 主题在 Hugo 内置的 shortcode 的基础上提供多个扩展的 shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:0:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"1 style 注意\rHugo extended 版本对于 style shortcode 是必需的.\rstyle shortcode 用来在你的文章中插入自定义样式. style shortcode 有两个位置参数. 第一个参数是自定义样式的内容. 它支持  SASS 中的嵌套语法, 并且 \u0026 指代这个父元素. 第二个参数是包裹你要更改样式的内容的 HTML 标签, 默认值是 div. 一个 style 示例: {{\u003c style \"text-align:right; strong{color:#00b1ff;}\" \u003e}} This is a **right-aligned** paragraph. {{\u003c /style \u003e}}\r呈现的输出效果如下: This is a right-aligned paragraph. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:1:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"2 link link shortcode 是 Markdown 链接语法 的替代. link shortcode 可以提供一些其它的功能并且可以在代码块中使用. 支持本地资源引用的完整用法. link shortcode 有以下命名参数: href [必需] (第一个位置参数) 链接的目标. content [可选] (第二个位置参数) 链接的内容, 默认值是 href 参数的值. 支持 Markdown 或者 HTML 格式. title [可选] (第三个位置参数) HTML a 标签 的 title 属性, 当悬停在链接上会显示的提示. rel [可选] HTML a 标签 的 rel 补充属性. class [可选] HTML a 标签 的 class 属性. 一个 link 示例: {{\u003c link \"https://assemble.io\" \u003e}} 或者 {{\u003c link href=\"https://assemble.io\" \u003e}} {{\u003c link \"mailto:contact@revolunet.com\" \u003e}} 或者 {{\u003c link href=\"mailto:contact@revolunet.com\" \u003e}} {{\u003c link \"https://assemble.io\" Assemble \u003e}} 或者 {{\u003c link href=\"https://assemble.io\" content=Assemble \u003e}}\r呈现的输出效果如下: https://assemble.io mailto:contact@revolunet.com Assemble 一个带有标题的 link 示例: {{\u003c link \"https://github.com/upstage/\" Upstage \"Visit Upstage!\" \u003e}} 或者 {{\u003c link href=\"https://github.com/upstage/\" content=Upstage title=\"Visit Upstage!\" \u003e}}\r呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:2:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"3 image image shortcode 是 figure shortcode 的替代. image shortcode 可以充分利用 lazysizes 和 lightGallery 两个依赖库. 支持本地资源引用的完整用法. image shortcode 有以下命名参数: src [必需] (第一个位置参数) 图片的 URL. alt [可选] (第二个位置参数) 图片无法显示时的替代文本, 默认值是 src 参数的值. 支持 Markdown 或者 HTML 格式. caption [可选] (第三个位置参数) 图片标题. 支持 Markdown 或者 HTML 格式. title [可选] 当悬停在图片上会显示的提示. class [可选] HTML figure 标签的 class 属性. src_s [可选] 图片缩略图的 URL, 用在画廊模式中, 默认值是 src 参数的值. src_l [可选] 高清图片的 URL, 用在画廊模式中, 默认值是 src 参数的值. height [可选] 图片的 height 属性. width [可选] 图片的 width 属性. linked [可选] 图片是否需要被链接, 默认值是 true. rel [可选] HTML a 标签 的 rel 补充属性, 仅在 linked 属性设置成 true 时有效. 一个 image 示例: {{\u003c image src=\"/images/lighthouse.jpg\" caption=\"Lighthouse (`image`)\" src_s=\"/images/lighthouse-small.jpg\" src_l=\"/images/lighthouse-large.jpg\" \u003e}}\r呈现的输出效果如下: Lighthouse (image)\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:3:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"4 admonition admonition shortcode 支持 12 种 帮助你在页面中插入提示的横幅. 支持 Markdown 或者 HTML 格式. 注意\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\radmonition shortcode 有以下命名参数: type [可选] (第一个位置参数) admonition 横幅的类型, 默认值是 note. title [可选] (第二个位置参数) admonition 横幅的标题, 默认值是 type 参数的值. open [可选] (第三个位置参数) 横幅内容是否默认展开, 默认值是 true. 一个 admonition 示例: {{\u003c admonition type=tip title=\"This is a tip\" open=false \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}} 或者 {{\u003c admonition tip \"This is a tip\" false \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}}\r呈现的输出效果如下: This is a tip\r一个 技巧 横幅\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:4:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5 mermaid mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能. 完整文档请查看页面 主题文档 - mermaid Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"6 echarts echarts shortcode 使用 ECharts 库提供数据可视化的功能. 完整文档请查看页面 主题文档 - echarts Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:6:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"7 mapbox mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能. 完整文档请查看页面 主题文档 - mapbox Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:7:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8 music music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器. 完整文档请查看页面 主题文档 - music Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"9 bilibili bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器. 完整文档请查看页面 主题文档 - bilibili Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:9:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10 typeit typeit shortcode 基于 TypeIt 库提供了打字动画. 完整文档请查看页面 主题文档 - typeit Shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"11 script script shortcode 用来在你的文章中插入  Javascript 脚本. 注意\r脚本内容可以保证在所有的第三方库加载之后按顺序执行. 所以你可以自由地使用第三方库.\r一个 script 示例: {{\u003c script \u003e}} console.log('Hello LoveIt!'); {{\u003c /script \u003e}}\r你可以在开发者工具的控制台中看到输出. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:11:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"12 raw raw shortcode 用来在你的文章中插入原始  HTML 内容. 一个 raw 示例: 行内公式: {{\u003c raw \u003e}}\\(\\mathbf{E}=\\sum_{i} \\mathbf{E}_{i}=\\mathbf{E}_{1}+\\mathbf{E}_{2}+\\mathbf{E}_{3}+\\cdots\\){{\u003c /raw \u003e}} 公式块: {{\u003c raw \u003e}} \\[ a=b+c \\\\ d+e=f \\] {{\u003c /raw \u003e}} 原始的带有 Markdown 语法的内容: {{\u003c raw \u003e}}**Hello**{{\u003c /raw \u003e}}\r呈现的输出效果如下: 行内公式: 公式块: 原始的带有 Markdown 语法的内容: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:12:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"13 person person shortcode 用来在你的文章中以 h-card 的格式插入个人网站链接. person shortcode 有以下命名参数: url [必需] (第一个位置参数) URL of the personal page. name [必需] (第二个位置参数) Name of the person. text [可选] (第三个位置参数) Text to display as hover tooltip of the link. picture [可选] (第四个位置参数) A picture to use as person’s avatar. nick [可选] Nickame of the person. 一个 person 示例: {{\u003c person url=\"https://evgenykuznetsov.org\" name=\"Evgeny Kuznetsov\" nick=\"nekr0z\" text=\"author of this shortcode\" picture=\"https://evgenykuznetsov.org/img/avatar.jpg\" \u003e}}\r呈现的输出效果为  Evgeny Kuznetsov (nekr0z). 一个使用通用图标的 person 示例: {{\u003c person \"https://dillonzq.com/\" Dillon \"author of the LoveIt theme\" \u003e}}\r呈现的输出效果为  Dillon. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:13:0","tags":["shortcodes"],"title":"主题文档 - 扩展 Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能.","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":" mermaid shortcode 使用 Mermaid 库提供绘制图表和流程图的功能. mermaid 是一个可以帮助你在文章中绘制图表和流程图的库, 类似 Markdown 的语法. 只需将你的 mermaid 代码插入 mermaid shortcode 中即可. ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"流程图 一个 流程图 mermaid 示例: {{\u003c mermaid \u003e}} graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"时序图 一个 时序图 mermaid 示例: {{\u003c mermaid \u003e}} sequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts \u003cbr/\u003eprevail... John--\u003eAlice: Great! John-\u003eBob: How about you? Bob--\u003eJohn: Jolly good! {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"甘特图 一个 甘特图 mermaid 示例: {{\u003c mermaid \u003e}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"类图 一个 类图 mermaid 示例: {{\u003c mermaid \u003e}} classDiagram Animal \u003c|-- Duck Animal \u003c|-- Fish Animal \u003c|-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() } {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:4:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"状态图 一个 状态图 mermaid 示例: {{\u003c mermaid \u003e}} stateDiagram-v2 [*] --\u003e Still Still --\u003e [*] Still --\u003e Moving Moving --\u003e Still Moving --\u003e Crash Crash --\u003e [*] {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:5:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"Git 图 一个 Git 图 mermaid 示例: {{\u003c mermaid \u003e}} gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:6:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"实体关系图 一个 实体关系图 mermaid 示例: {{\u003c mermaid \u003e}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:7:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"用户体验旅程图 一个 用户体验旅程图 mermaid 示例: {{\u003c mermaid \u003e}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:8:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"饼图 一个 饼图 mermaid 示例: {{\u003c mermaid \u003e}} pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:9:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"依赖图 一个 依赖图 mermaid 示例: {{\u003c mermaid \u003e}} requirementDiagram requirement test_req { id: 1 text: the test text. risk: high verifymethod: test } element test_entity { type: simulation } test_entity - satisfies -\u003e test_req {{\u003c /mermaid \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mermaid-shortcode/:10:0","tags":["shortcodes"],"title":"主题文档 - mermaid Shortcode","uri":"/theme-documentation-mermaid-shortcode/"},{"categories":["documentation"],"content":"echarts shortcode 使用 ECharts 库提供数据可视化的功能.","date":"2020-03-03","objectID":"/theme-documentation-echarts-shortcode/","tags":["shortcodes"],"title":"主题文档 - echarts Shortcode","uri":"/theme-documentation-echarts-shortcode/"},{"categories":["documentation"],"content":"echarts shortcode 使用 ECharts 库提供数据可视化的功能. ECharts 是一个帮助你生成交互式数据可视化的库. ECharts 提供了常规的 折线图, 柱状图, 散点图, 饼图, K线图, 用于统计的 盒形图, 用于地理数据可视化的 地图, 热力图, 线图, 用于关系数据可视化的 关系图, treemap, 旭日图, 多维数据可视化的 平行坐标, 还有用于 BI 的 漏斗图, 仪表盘, 并且支持图与图之间的混搭. 只需在 echarts shortcode 中以 JSON/YAML/TOML格式插入 ECharts 选项即可. 一个 JSON 格式的 echarts 示例: {{\u003c echarts \u003e}} { \"title\": { \"text\": \"折线统计图\", \"top\": \"2%\", \"left\": \"center\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"邮件营销\", \"联盟广告\", \"视频广告\", \"直接访问\", \"搜索引擎\"], \"top\": \"10%\" }, \"grid\": { \"left\": \"5%\", \"right\": \"5%\", \"bottom\": \"5%\", \"top\": \"20%\", \"containLabel\": true }, \"toolbox\": { \"feature\": { \"saveAsImage\": { \"title\": \"保存为图片\" } } }, \"xAxis\": { \"type\": \"category\", \"boundaryGap\": false, \"data\": [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"] }, \"yAxis\": { \"type\": \"value\" }, \"series\": [ { \"name\": \"邮件营销\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [120, 132, 101, 134, 90, 230, 210] }, { \"name\": \"联盟广告\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [220, 182, 191, 234, 290, 330, 310] }, { \"name\": \"视频广告\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [150, 232, 201, 154, 190, 330, 410] }, { \"name\": \"直接访问\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [320, 332, 301, 334, 390, 330, 320] }, { \"name\": \"搜索引擎\", \"type\": \"line\", \"stack\": \"总量\", \"data\": [820, 932, 901, 934, 1290, 1330, 1320] } ] } {{\u003c /echarts \u003e}}\r一个 YAML 格式的 echarts 示例: {{\u003c echarts \u003e}} title: text: 折线统计图 top: 2% left: center tooltip: trigger: axis legend: data: - 邮件营销 - 联盟广告 - 视频广告 - 直接访问 - 搜索引擎 top: 10% grid: left: 5% right: 5% bottom: 5% top: 20% containLabel: true toolbox: feature: saveAsImage: title: 保存为图片 xAxis: type: category boundaryGap: false data: - 周一 - 周二 - 周三 - 周四 - 周五 - 周六 - 周日 yAxis: type: value series: - name: 邮件营销 type: line stack: 总量 data: - 120 - 132 - 101 - 134 - 90 - 230 - 210 - name: 联盟广告 type: line stack: 总量 data: - 220 - 182 - 191 - 234 - 290 - 330 - 310 - name: 视频广告 type: line stack: 总量 data: - 150 - 232 - 201 - 154 - 190 - 330 - 410 - name: 直接访问 type: line stack: 总量 data: - 320 - 332 - 301 - 334 - 390 - 330 - 320 - name: 搜索引擎 type: line stack: 总量 data: - 820 - 932 - 901 - 934 - 1290 - 1330 - 1320 {{\u003c /echarts \u003e}}\r一个 TOML 格式的 echarts 示例: {{\u003c echarts \u003e}} [title] text = \"折线统计图\" top = \"2%\" left = \"center\" [tooltip] trigger = \"axis\" [legend] data = [ \"邮件营销\", \"联盟广告\", \"视频广告\", \"直接访问\", \"搜索引擎\" ] top = \"10%\" [grid] left = \"5%\" right = \"5%\" bottom = \"5%\" top = \"20%\" containLabel = true [toolbox] [toolbox.feature] [toolbox.feature.saveAsImage] title = \"保存为图片\" [xAxis] type = \"category\" boundaryGap = false data = [ \"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\" ] [yAxis] type = \"value\" [[series]] name = \"邮件营销\" type = \"line\" stack = \"总量\" data = [ 120.0, 132.0, 101.0, 134.0, 90.0, 230.0, 210.0 ] [[series]] name = \"联盟广告\" type = \"line\" stack = \"总量\" data = [ 220.0, 182.0, 191.0, 234.0, 290.0, 330.0, 310.0 ] [[series]] name = \"视频广告\" type = \"line\" stack = \"总量\" data = [ 150.0, 232.0, 201.0, 154.0, 190.0, 330.0, 410.0 ] [[series]] name = \"直接访问\" type = \"line\" stack = \"总量\" data = [ 320.0, 332.0, 301.0, 334.0, 390.0, 330.0, 320.0 ] [[series]] name = \"搜索引擎\" type = \"line\" stack = \"总量\" data = [ 820.0, 932.0, 901.0, 934.0, 1290.0, 1330.0, 1320.0 ] {{\u003c /echarts \u003e}}\r呈现的输出效果如下: echarts shortcode 还有以下命名参数: width [可选] (第一个位置参数) 数据可视化的宽度, 默认值是 100%. height [可选] (第二个位置参数) 数据可视化的高度, 默认值是 30rem. ","date":"2020-03-03","objectID":"/theme-documentation-echarts-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - echarts Shortcode","uri":"/theme-documentation-echarts-shortcode/"},{"categories":["documentation"],"content":"mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能.","date":"2020-03-03","objectID":"/theme-documentation-mapbox-shortcode/","tags":["shortcodes"],"title":"主题文档 - mapbox Shortcode","uri":"/theme-documentation-mapbox-shortcode/"},{"categories":["documentation"],"content":" mapbox shortcode 使用 Mapbox GL JS 库提供互动式地图的功能. Mapbox GL JS 是一个 JavaScript 库，它使用 WebGL, 以 vector tiles 和 Mapbox styles 为来源, 将它们渲染成互动式地图. mapbox shortcode 有以下命名参数来使用 Mapbox GL JS: lng [必需] (第一个位置参数) 地图初始中心点的经度, 以度为单位. lat [必需] (第二个位置参数) 地图初始中心点的纬度, 以度为单位. zoom [可选] (第三个位置参数) 地图的初始缩放级别, 默认值是 10. marked [可选] (第四个位置参数) 是否在地图的初始中心点添加图钉, 默认值是 true. light-style [可选] (第五个位置参数) 浅色主题的地图样式, 默认值是前置参数或者网站配置中设置的值. dark-style [可选] (第六个位置参数) 深色主题的地图样式, 默认值是前置参数或者网站配置中设置的值. navigation [可选] 是否添加 NavigationControl, 默认值是前置参数或者网站配置中设置的值. geolocate [可选] 是否添加 GeolocateControl, 默认值是前置参数或者网站配置中设置的值. scale [可选] 是否添加 ScaleControl, 默认值是前置参数或者网站配置中设置的值. fullscreen [可选] 是否添加 FullscreenControl, 默认值是前置参数或者网站配置中设置的值. width [可选] 地图的宽度, 默认值是 100%. height [可选] 地图的高度, 默认值是 20rem. 一个简单的 mapbox 示例: {{\u003c mapbox 121.485 31.233 12 \u003e}} 或者 {{\u003c mapbox lng=121.485 lat=31.233 zoom=12 \u003e}}\r呈现的输出效果如下: 一个带有自定义样式的 mapbox 示例: {{\u003c mapbox -122.252 37.453 10 false \"mapbox://styles/mapbox/streets-zh-v1?optimize=true\" \u003e}} 或者 {{\u003c mapbox lng=-122.252 lat=37.453 zoom=10 marked=false light-style=\"mapbox://styles/mapbox/streets-zh-v1?optimize=true\" \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-mapbox-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - mapbox Shortcode","uri":"/theme-documentation-mapbox-shortcode/"},{"categories":["documentation"],"content":"music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器.","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"music shortcode 基于 APlayer 和 MetingJS 库提供了一个内嵌的响应式音乐播放器. 有三种方式使用 music shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"1 自定义音乐 URL 支持本地资源引用的完整用法. music shortcode 有以下命名参数来使用自定义音乐 URL: server [必需] 音乐的链接. type [可选] 音乐的名称. artist [可选] 音乐的创作者. cover [可选] 音乐的封面链接. 一个使用自定义音乐 URL 的 music 示例: {{\u003c music url=\"/music/Wavelength.mp3\" name=Wavelength artist=oldmanyoung cover=\"/images/Wavelength.jpg\" \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"2 音乐平台 URL 的自动识别 music shortcode 有一个命名参数来使用音乐平台 URL 的自动识别: auto [必需]] (第一个位置参数) 用来自动识别的音乐平台 URL, 支持 netease, tencent 和 xiami 平台. 一个使用音乐平台 URL 的自动识别的 music 示例: {{\u003c music auto=\"https://music.163.com/#/playlist?id=60198\" \u003e}} 或者 {{\u003c music \"https://music.163.com/#/playlist?id=60198\" \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"3 自定义音乐平台, 类型和 ID music shortcode 有以下命名参数来使用自定义音乐平台: server [必需] (第一个位置参数) [netease, tencent, kugou, xiami, baidu] 音乐平台. type [必需] (第二个位置参数) [song, playlist, album, search, artist] 音乐类型. id [必需] (第三个位置参数) 歌曲 ID, 或者播放列表 ID, 或者专辑 ID, 或者搜索关键词, 或者创作者 ID. 一个使用自定义音乐平台的 music 示例: {{\u003c music server=\"netease\" type=\"song\" id=\"1868553\" \u003e}} 或者 {{\u003c music netease song 1868553 \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["documentation"],"content":"4 其它参数 music shortcode 有一些可以应用于以上三种方式的其它命名参数: theme [可选] 音乐播放器的主题色, 默认值是 #448aff. fixed [可选] 是否开启固定模式, 默认值是 false. mini [可选] 是否开启迷你模式, 默认值是 false. autoplay [可选] 是否自动播放音乐, 默认值是 false. volume [可选] 第一次打开播放器时的默认音量, 会被保存在浏览器缓存中, 默认值是 0.7. mutex [可选] 是否自动暂停其它播放器, 默认值是 true. music shortcode 还有一些只适用于音乐列表方式的其它命名参数: loop [可选] [all, one, none] 音乐列表的循环模式, 默认值是 none. order [可选] [list, random] 音乐列表的播放顺序, 默认值是 list. list-folded [可选] 初次打开的时候音乐列表是否折叠, 默认值是 false. list-max-height [可选] 音乐列表的最大高度, 默认值是 340px. ","date":"2020-03-03","objectID":"/theme-documentation-music-shortcode/:4:0","tags":["shortcodes"],"title":"主题文档 - music Shortcode","uri":"/theme-documentation-music-shortcode/"},{"categories":["projects"],"content":"bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器.","date":"2020-03-03","objectID":"/theme-documentation-bilibili-shortcode/","tags":["shortcodes"],"title":"主题文档 - bilibili Shortcode","uri":"/theme-documentation-bilibili-shortcode/"},{"categories":["projects"],"content":" bilibili shortcode 提供了一个内嵌的用来播放 bilibili 视频的响应式播放器. 如果视频只有一个部分, 则仅需要视频的 BV id, 例如: https://www.bilibili.com/video/BV1Sx411T7QQ\r一个 bilibili 示例: {{\u003c bilibili BV1Sx411T7QQ \u003e}} 或者 {{\u003c bilibili id=BV1Sx411T7QQ \u003e}}\r呈现的输出效果如下: 如果视频包含多个部分, 则除了视频的 BV id 之外, 还需要 p, 默认值为 1, 例如: https://www.bilibili.com/video/BV1TJ411C7An?p=3\r一个带有 p 参数的 bilibili 示例: {{\u003c bilibili BV1TJ411C7An 3 \u003e}} 或者 {{\u003c bilibili id=BV1TJ411C7An p=3 \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-bilibili-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - bilibili Shortcode","uri":"/theme-documentation-bilibili-shortcode/"},{"categories":["documentation"],"content":"typeit shortcode 基于 TypeIt 库提供了打字动画.","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"typeit shortcode 基于 TypeIt 库提供了打字动画. 只需将你需要打字动画的内容插入 typeit shortcode 中即可. ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:0:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"1 简单内容 允许使用 Markdown 格式的简单内容, 并且 不包含 富文本的块内容, 例如图像等等… 一个 typeit 示例: {{\u003c typeit \u003e}} 这一个带有基于 [TypeIt](https://typeitjs.com/) 的 **打字动画** 的 *段落*... {{\u003c /typeit \u003e}}\r呈现的输出效果如下: 另外, 你也可以自定义 HTML 标签. 一个带有 h4 标签的 typeit 示例: {{\u003c typeit tag=h4 \u003e}} 这一个带有基于 [TypeIt](https://typeitjs.com/) 的 **打字动画** 的 *段落*... {{\u003c /typeit \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:1:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"2 代码内容 代码内容也是允许的, 并且通过使用参数 code 指定语言类型可以实习语法高亮. 一个带有 code 参数的 typeit 示例: {{\u003c typeit code=java \u003e}} public class HelloWorld { public static void main(String []args) { System.out.println(\"Hello World\"); } } {{\u003c /typeit \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:2:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["documentation"],"content":"3 分组内容 默认情况下, 所有打字动画都是同时开始的. 但是有时你可能需要按顺序开始一组 typeit 内容的打字动画. 一组具有相同 group 参数值的 typeit 内容将按顺序开始打字动画. 一个带有 group 参数的 typeit 示例: {{\u003c typeit group=paragraph \u003e}} **首先**, 这个段落开始 {{\u003c /typeit \u003e}} {{\u003c typeit group=paragraph \u003e}} **然后**, 这个段落开始 {{\u003c /typeit \u003e}}\r呈现的输出效果如下: ","date":"2020-03-03","objectID":"/theme-documentation-typeit-shortcode/:3:0","tags":["shortcodes"],"title":"主题文档 - typeit Shortcode","uri":"/theme-documentation-typeit-shortcode/"},{"categories":["工具 info"],"content":"我的 放博客的GitHub仓库 loveit相关 hugoloveit官方教程 别人用loveit做的简单博客示例——渣渣的夏天 浙大同学的博客，挺好看的 野生教程 loveit野生教程，有可参考的 GitHub pages + hugo 搭个人博客的教程 hugo教程 图标 fontawesome iconfont阿里巴巴 ","date":"2025-08-26","objectID":"/info-documentation-loveit/:0:0","tags":["blog"],"title":"loveit的博客构建","uri":"/info-documentation-loveit/"},{"categories":null,"content":"关于 LoveIt","date":"2019-08-02","objectID":"/about/","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt\r","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特性 ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex 和 Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"2019-08-02","objectID":"/about/:1:1","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 81 种社交链接  支持多达 27 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"2019-08-02","objectID":"/about/:1:2","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"2019-08-02","objectID":"/about/:1:3","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"关于 LoveIt","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： modern-normalize Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"关于 LoveIt","uri":"/about/"}]